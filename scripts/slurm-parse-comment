#!/usr/bin/env python
"""Parses the comment from a Slurm job ID.

If called without any arguments, this script will cache the comments for all
jobs from today. If called with a job ID, it will print the comment for that
job ID.
"""

import argparse
import functools
import json
import os
import re
import shutil
import subprocess
import sys
import threading
import time
from pathlib import Path
from typing import Dict, List, Optional

# This prefix shouldn't be used in the slurm comments.
CACHE_TIME_PREFIX = "Cache Time: "


def get_job_id_cache_path() -> Path:
    """Gets the path to the job ID cache.

    Returns:
        The path to the job ID cache.
    """

    return Path.home() / ".cache" / "slurm-log-files" / "comments.json"


@functools.lru_cache
def load_job_id_cache() -> Dict[str, List[str]]:
    """Loads the job ID cache.

    Returns:
        The job ID cache.
    """

    cache_path = get_job_id_cache_path()
    if not cache_path.exists():
        return {}
    with open(cache_path, "r", encoding="utf-8") as f:
        return json.load(f)


def save_job_id_cache(job_ids: Dict[str, List[str]]) -> None:
    """Saves the job ID cache.

    Args:
        job_ids: The job ID cache.
    """

    cache_path = get_job_id_cache_path()
    cache_path.parent.mkdir(exist_ok=True, parents=True)
    with open(cache_path, "w", encoding="utf-8") as f:
        json.dump(job_ids, f, indent=2)


def save_to_cache(job_id: str, comments: List[str]) -> None:
    """Saves the job ID and comment to the cache.

    Args:
        job_id: The job ID.
        comments: The comments.
    """

    job_ids = load_job_id_cache()
    current_time = time.time()
    job_ids[job_id] = comments + [f"{CACHE_TIME_PREFIX}{current_time}"]
    save_job_id_cache(job_ids)


def cleanup(proc: subprocess.Popen, command: List[str]) -> None:
    """Cleans up the process if it times out.

    Args:
        proc: The process.
        command: The command.
    """

    print(f"Timeout while calling {' '.join(command)}", file=sys.stderr)
    proc.kill()


def run_command(cmd: List[str], timeout: int) -> str:
    """Runs a command and returns the output.

    Args:
        cmd: The command to run.
        timeout: The timeout in seconds.

    Returns:
        The output of the command.
    """

    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    timer = threading.Timer(timeout, cleanup, [process, cmd])

    try:
        timer.start()
        stdout, stderr = process.communicate()
        sys.stderr.buffer.write(stderr)
    finally:
        timer.cancel()

    return stdout.decode("utf-8")


def parse_comment(job_id: str) -> List[str]:
    """Parses the comment for a job ID.

    Args:
        job_id: The job ID.

    Returns:
        The comment.

    Raises:
        RuntimeError: If the job ID is invalid.
    """

    job_id_cache = load_job_id_cache()
    if job_id in job_id_cache and job_id_cache[job_id]:
        return job_id_cache[job_id]

    # Attempts to parse job ID using `sacct`.
    stdout = run_command(["sacct", "--jobs", job_id, "--format", "Comment%10000"], 10)
    comment_re = re.search("--- \n(.+?)\n", stdout, re.MULTILINE)
    if comment_re is not None:
        comments = [c.strip() for c in re.split(r";\s*", comment_re.group(1).strip()) if c.strip()]
        if comments:
            save_to_cache(job_id, comments)
            return comments

    # Attempts to parse job ID using `squeue`.
    stdout = run_command(["squeue", "--jobs", job_id, "--format", "%k"], 10)
    comment_re = re.search(r"COMMENT\n(.+)\n", stdout)
    if comment_re is not None:
        comments = [c.strip() for c in re.split(r";\s*", comment_re.group(1).strip()) if c.strip()]
        if comments:
            save_to_cache(job_id, comments)
            return comments

    raise RuntimeError(f"Failed to parse comment for {job_id}: {stdout}")


def is_valid_time(comments: List[str], current_time: float) -> bool:
    """Checks if the comments were recently added.

    Args:
        comments: The comments.
        current_time: The current time.

    Returns:
        True if the comments were recently added.
    """

    if not any(c.startswith(CACHE_TIME_PREFIX) for c in comments):
        return False
    time_str = [c for c in comments if c.startswith(CACHE_TIME_PREFIX)][0][len(CACHE_TIME_PREFIX) :]
    return current_time - float(time_str) < 60 * 60 * 24 * 30  # 1 month


def cache_today_dirs() -> None:
    """Caches the comments for all jobs from today."""

    log_dir_str = os.environ.get("LOG_DIR")
    if log_dir_str is None:
        return
    log_dir = Path(log_dir_str)
    if not log_dir.is_dir():
        return

    def parse_comment_from_file(sbatch_file: Path) -> Optional[List[str]]:
        with open(sbatch_file, "r", encoding="utf-8") as f:
            for line in f:
                comment_match = re.match(r"#SBATCH --comment='(.+)'\n", line)
                if comment_match is not None:
                    comment_str = comment_match.group(1)
                    return [c.strip() for c in re.split(r";\s*", comment_str) if c.strip()]
            return None

    # Gets all job IDs from today's log directory.
    new_job_ids: Dict[str, List[str]] = {}
    for sbatch_file in log_dir.glob("today/*/run_*/sbatch.sh"):
        stderr_files = list(sbatch_file.parent.glob("logs/slurm_err.*.txt"))
        if not stderr_files:
            continue

        # Gets the job ID and comments.
        comment = parse_comment_from_file(sbatch_file)
        if comment is None:
            continue
        stderr_job_ids = [stderr_file.name.split(".", 1)[1].split(".", 1)[0] for stderr_file in stderr_files]
        for stderr_job_id in stderr_job_ids:
            new_job_ids[stderr_job_id] = comment

    current_time = time.time()
    new_job_ids = {k: v + [f"{CACHE_TIME_PREFIX}{current_time}"] for k, v in new_job_ids.items()}

    # Gets job IDs from the cache, removing any that are older than 1 month.
    job_ids = load_job_id_cache()
    job_ids = {k: v for k, v in job_ids.items() if is_valid_time(v, current_time)}
    job_ids.update(new_job_ids)

    save_job_id_cache(job_ids)


def cache_running_jobs() -> None:
    """Caches the comments for all running jobs."""

    if shutil.which("squeue") is None:
        return

    stdout = run_command(["squeue", "--me", "--format", "%i=%k"], 10)
    new_job_ids_strs = dict(line.strip().split("=", 1) for line in stdout.strip().split("\n")[1:])
    if not new_job_ids_strs:
        return
    current_time = time.time()
    new_job_ids = {
        k: re.split(r";\s*", v) + [f"{CACHE_TIME_PREFIX}{current_time}"] for k, v in new_job_ids_strs.items()
    }

    # Gets job IDs from the cache, removing any that are older than 1 month.
    job_ids = load_job_id_cache()
    job_ids = {k: v for k, v in job_ids.items() if is_valid_time(v, current_time)}
    job_ids.update(new_job_ids)

    save_job_id_cache(job_ids)


def main() -> None:
    """Parses the slurm comment from a job ID."""

    parser = argparse.ArgumentParser(description="Parses the slurm comment for a job ID")
    parser.add_argument("job_id", nargs="?", help="The job ID to look up")
    parser.add_argument("-p", "--prefix", help="Only print lines with this prefix")
    parser.add_argument(
        "-r",
        "--running-jobs",
        default=False,
        action="store_true",
        help="If set, caches the comments for all running jobs",
    )
    parser.add_argument("-n", "--no-retry", default=False, action="store_true", help="If set, don't retry on failure")
    parser.add_argument("-t", "--num-retries", type=int, default=1, help="The number of times to retry on failure")
    args = parser.parse_args()

    if args.job_id is None:
        if args.running_jobs:
            cache_running_jobs()
        else:
            cache_today_dirs()

    else:
        for _ in range(args.num_retries + 1):
            try:
                parsed_comment = parse_comment(args.job_id)
                if args.prefix is not None:
                    parsed_comment = [p[len(args.prefix) :] for p in parsed_comment if p.startswith(args.prefix)]
                print("\n".join(parsed_comment))

            except RuntimeError as exc:
                if args.no_retry:
                    raise

                print(exc, file=sys.stderr)
                if args.running_jobs:
                    cache_running_jobs()
                else:
                    cache_today_dirs()
                time.sleep(5)


if __name__ == "__main__":
    main()
